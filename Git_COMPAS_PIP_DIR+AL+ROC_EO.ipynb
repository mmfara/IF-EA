{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi382WOiBjR/1ORAACbnU9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmfara/IF-EA/blob/main/Git_COMPAS_PIP_DIR%2BAL%2BROC_EO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up8IIJmU-eSa"
      },
      "outputs": [],
      "source": [
        "# Data handling and visualization\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# AIF360 fairness library\n",
        "from aif360.datasets import StandardDataset, BinaryLabelDataset\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing\n",
        "from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.distortion_functions import get_distortion_adult\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.opt_tools import OptTools\n",
        "from aif360.algorithms.inprocessing import ExponentiatedGradientReduction, AdversarialDebiasing\n",
        "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
        "from aif360.algorithms.postprocessing.reject_option_classification import RejectOptionClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = pd.read_csv(COMPAS dataset)\n",
        "\n",
        "X = data.drop(['two_year_recid'], axis =1)\n",
        "y = data[['two_year_recid']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=101, shuffle =True, stratify = y)\n",
        "\n",
        "dataset_train = pd.concat([X_train, y_train], axis=1)\n",
        "dataset_test = pd.concat([X_test, y_test], axis=1)\n",
        "\n",
        "dataset_train = dataset_train.reset_index(drop=True)\n",
        "dataset_test = dataset_test.reset_index(drop=True)\n",
        "\n",
        "# Define favorable and unfavorable labels\n",
        "favorable_label = 1\n",
        "unfavorable_label = 0\n",
        "\n",
        "# Define protected attribute names and privileged group\n",
        "protected_attribute_names = ['race']\n",
        "privileged_group = [{'race': 1}]\n",
        "unprivileged_group = [{'race' : 0}]\n",
        "\n",
        "dir = DisparateImpactRemover(repair_level=1, sensitive_attribute=\"race\")\n",
        "\n",
        "# Convert the training and test set to Binary label datasets\n",
        "dataset_train_bld = BinaryLabelDataset(df=dataset_train,\n",
        "                             label_names=['two_year_recid'],\n",
        "                             favorable_label=favorable_label,\n",
        "                             unfavorable_label=unfavorable_label,\n",
        "                             protected_attribute_names=protected_attribute_names,\n",
        "                             privileged_protected_attributes=privileged_group)\n",
        "\n",
        "dataset_test_bld = BinaryLabelDataset(df=dataset_test,\n",
        "                                      label_names=['two_year_recid'],\n",
        "                                      favorable_label=favorable_label,\n",
        "                                      unfavorable_label=unfavorable_label,\n",
        "                                      protected_attribute_names=protected_attribute_names,\n",
        "                                      privileged_protected_attributes=privileged_group)\n",
        "\n",
        "# Fit and transform the training data\n",
        "dir_dataset = dir.fit_transform(dataset_train_bld)\n",
        "\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "# Use a different scope name if re-running the fitting process\n",
        "debias_model = AdversarialDebiasing(privileged_groups=[{'race': 1}],\n",
        "                                    unprivileged_groups=[{'race': 0}],\n",
        "                                    scope_name='debias_model_unique_scope',  # Change the scope name for each new instance\n",
        "                                    debias=True,\n",
        "                                    sess=sess,\n",
        "                                    seed = 101)\n",
        "\n",
        "debias_model.fit(dir_dataset)\n",
        "\n",
        "# Predict with the debias_model\n",
        "predictions = debias_model.predict(dataset_test_bld)\n",
        "\n",
        "from aif360.metrics import ClassificationMetric\n",
        "\n",
        "# Assuming dataset_test_bld is the original test dataset with true labels\n",
        "# and predictions is the dataset with your model's predictions\n",
        "metric_predicted_dataset = ClassificationMetric(dataset_test_bld, predictions,\n",
        "                              unprivileged_groups=unprivileged_group,\n",
        "                              privileged_groups=privileged_group)\n",
        "\n",
        "\n",
        "# Print metrics\n",
        "print(\"Performance with fairness intervention (Reject Option Classification):\")\n",
        "print(\"Accuracy: {:.6f}\".format(metric_predicted_dataset.accuracy()))\n",
        "print(\"Disparate Impact: {:.6f}\".format(metric_predicted_dataset.disparate_impact()))\n",
        "print(\"Mean Difference: {:.6f}\".format(metric_predicted_dataset.mean_difference()))\n",
        "print(f\"Statistical Parity Difference: {metric_predicted_dataset.statistical_parity_difference()}\")\n",
        "print(f\"Disparate Impact: {metric_predicted_dataset.disparate_impact()}\")\n",
        "print(f\"Equal Opportunity Difference: {metric_predicted_dataset.equal_opportunity_difference()}\")\n",
        "print(f\"Predictive Equality: {metric_predicted_dataset.false_positive_rate_difference()}\")\n",
        "\n",
        "#Implementing the ROC on AL output\n",
        "ROC = RejectOptionClassification(unprivileged_groups=unprivileged_group,\n",
        "                                  privileged_groups=privileged_group, metric_name=\"Equal opportunity difference\",\n",
        "                                 low_class_thresh=0.3, high_class_thresh=0.8, num_class_thresh=100, num_ROC_margin=50,\n",
        "                                 metric_ub=0.05, metric_lb=-0.05)\n",
        "\n",
        "test_predictions = ROC.fit_predict(dataset_test_bld, predictions)\n",
        "\n",
        "# Evaluate the classifier's performance with ROC intervention\n",
        "metric_with_fairness = ClassificationMetric(\n",
        "    dataset_test_bld,\n",
        "    test_predictions,\n",
        "    unprivileged_groups=unprivileged_group,\n",
        "    privileged_groups=privileged_group\n",
        ")\n",
        "\n",
        "print(\"Performance with fairness intervention (Reject Option Classification):\")\n",
        "print(\"Accuracy: {:.2f}\".format(metric_with_fairness.accuracy()))\n",
        "# Calculate fairness metrics\n",
        "statistical_parity_difference = metric_with_fairness.statistical_parity_difference()\n",
        "disparate_impact = metric_with_fairness.disparate_impact()\n",
        "equal_opportunity_difference = metric_with_fairness.equal_opportunity_difference()\n",
        "\n",
        "# Print out the metrics\n",
        "print(f\"Statistical Parity Difference: {statistical_parity_difference}\")\n",
        "print(f\"Disparate Impact: {disparate_impact}\")\n",
        "print(f\"Equal Opportunity Difference: {equal_opportunity_difference}\")\n",
        "print(\"Difference in True Positive Rates (Unprivileged - Privileged) = %f\" % metric_with_fairness.true_positive_rate_difference())\n",
        "print(\"Difference in False Positive Rates (Unprivileged - Privileged) = %f\" % metric_with_fairness.false_positive_rate_difference())\n",
        "\n",
        "#Implementing the EO on AL output\n",
        "# Initialize EqualizedOdds\n",
        "eo = EqOddsPostprocessing(unprivileged_groups=unprivileged_group,\n",
        "                                  privileged_groups=privileged_group, seed = 101)\n",
        "\n",
        "# Fit the EqualizedOdds model\n",
        "test_predictions = eo.fit_predict(dataset_test_bld, predictions)\n",
        "\n",
        "# Evaluate the classifier's performance with fairness intervention\n",
        "metric_with_fairness = ClassificationMetric(\n",
        "    dataset_test_bld,\n",
        "    test_predictions,\n",
        "    unprivileged_groups=unprivileged_group,\n",
        "    privileged_groups=privileged_group\n",
        ")\n",
        "\n",
        "print(\"Performance with fairness intervention (Equalized Odds):\")\n",
        "print(\"Accuracy: {:.6f}\".format(metric_with_fairness.accuracy()))\n",
        "# Calculate fairness metrics\n",
        "statistical_parity_difference = metric_with_fairness.statistical_parity_difference()\n",
        "disparate_impact = metric_with_fairness.disparate_impact()\n",
        "equal_opportunity_difference = metric_with_fairness.equal_opportunity_difference()\n",
        "# Print out the metrics\n",
        "print(f\"Statistical Parity Difference: {statistical_parity_difference}\")\n",
        "print(f\"Disparate Impact: {disparate_impact}\")\n",
        "print(f\"Equal Opportunity Difference: {equal_opportunity_difference}\")\n",
        "# Print out the equality of odds metrics\n",
        "print(\"Difference in True Positive Rates (Unprivileged - Privileged) = %f\" % metric_with_fairness.true_positive_rate_difference())\n",
        "print(\"Difference in False Positive Rates (Unprivileged - Privileged) = %f\" % metric_with_fairness.false_positive_rate_difference())\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}